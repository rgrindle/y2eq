{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import sympy\n",
    "from sympy import lambdify\n",
    "from eqlearner.dataset.univariate.datasetcreator import DatasetCreator\n",
    "from eqlearner.dataset.processing import tokenization\n",
    "from sympy import sin, Symbol, log, exp \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torchtext.datasets \n",
    "from torchtext.data import Field, BucketIterator\n",
    "import matplotlib.pyplot as plt\n",
    "#from eq_learner.processing import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    r\"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Arguments:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *tensors):\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index].cuda() for tensor in self.tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "    \n",
    "def dataset_loader(train_dataset,test_dataset, batch_size = 1024, valid_size = 0.20):\n",
    "    num_train = len(train_dataset)\n",
    "    num_test_h = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    test_idx_h = list(range(num_test_h))\n",
    "    np.random.shuffle(test_idx_h)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "        sampler=train_sampler, num_workers=0)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "        sampler=valid_sampler, num_workers=0)\n",
    "    test_loader_h = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
    "        shuffle=False, num_workers=0)\n",
    "    return train_loader, valid_loader, test_loader_h, valid_idx, train_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sin(x)**4+sin(x)**2+1+exp(sin(x)**3+sin(x)**2+sin(x)+1)\n",
      "[12  2  5  1  6  7 19  9  2  5  1  6  7 17  9 16  9  3  5  2  5  1  6  7\n",
      " 18  9  2  5  1  6  7 17  9  2  5  1  6  9 16  6 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x133dca2e0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = True\n",
    "scaler = MinMaxScaler()\n",
    "x = Symbol('x')\n",
    "basis_functions = [x,sin,log,exp]\n",
    "support = np.arange(0.1,3.1,0.1)\n",
    "fun_generator = DatasetCreator(basis_functions,max_linear_terms=1, max_binomial_terms=1,max_compositions=1,max_N_terms=1,division_on=False,  random_terms=True, constants_enabled = True, constant_intervals_ext=[(-10,1),(1,10)], constant_intervals_int = [(1,3)])\n",
    "string, dictionary = fun_generator.generate_batch(support,1, X_noise=False, Y_noise=0)\n",
    "print(tokenization.get_string(tokenization.pipeline(dictionary)[0]))\n",
    "print(tokenization.pipeline(dictionary)[0])\n",
    "plt.plot(string[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eqs = 500 #Number of equations needed for training\n",
    "test_eqs = 100\n",
    "scale = True\n",
    "scaler = MinMaxScaler()\n",
    "x = Symbol('x')\n",
    "basis_functions = [x,sin,log,exp]\n",
    "support = np.arange(0.1,3.1,0.1)\n",
    "fun_generator = DatasetCreator(basis_functions,max_linear_terms=1, max_binomial_terms=1,max_compositions=1,max_N_terms=0,division_on=False,  random_terms=True, constants_enabled = True, constant_intervals_ext=[(-3,1),(1,3)], constant_intervals_int = [(1,3)])\n",
    "x_train = []\n",
    "y_train = []\n",
    "cnt = 0\n",
    "cond = True\n",
    "while cond == True:\n",
    "    string, dictionary =  fun_generator.generate_batch(support,1, X_noise=False, Y_noise=0)\n",
    "    if np.all(string[0][1] == 0) == False:\n",
    "        if np.max(string[0][1])<1000 and np.min(string[0][1])>-1000 and tokenization.get_string(tokenization.pipeline(dictionary)[0])[-1] != '+': \n",
    "            x_train.append(string[0][1])\n",
    "            y_train.append(torch.Tensor((tokenization.pipeline(dictionary)[0])))\n",
    "#             print(tokenization.get_string(tokenization.pipeline(dictionary)[0]))\n",
    "            cnt+=1\n",
    "            #print(cnt)\n",
    "    if cnt == train_eqs:\n",
    "        cond = False\n",
    "scaler.fit(np.array(x_train).T)\n",
    "x_train_n = scaler.transform(np.array(x_train).T)\n",
    "x_train_n = torch.Tensor(x_train_n)\n",
    "l = [len(y) for y in y_train]\n",
    "q = np.max(l)\n",
    "y_train_p = torch.zeros(len(y_train),q)\n",
    "for i,y in enumerate(y_train):\n",
    "    y_train_p[i,:] = torch.cat([y,torch.zeros(q-y.shape[0])])\n",
    "train_data = TensorDataset(x_train_n.T,y_train_p.long())\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "cnt = 0\n",
    "cond = True\n",
    "while cond == True:\n",
    "    string, dictionary =  fun_generator.generate_batch(support,1, X_noise=False, Y_noise=0)\n",
    "    if np.all(string[0][1] == 0) == False:\n",
    "        if np.max(string[0][1])<1000 and np.min(string[0][1])>-1000 and tokenization.get_string(tokenization.pipeline(dictionary)[0])[-1] != '+': \n",
    "            x_test.append(string[0][1])\n",
    "            y_test.append(torch.Tensor((tokenization.pipeline(dictionary)[0])))\n",
    "            #print(tokenization.get_string(tokenization.pipeline(dictionary)[0]))\n",
    "            cnt+=1\n",
    "            #print(cnt)\n",
    "    if cnt == test_eqs:\n",
    "        cond = False\n",
    "scaler = MinMaxScaler()\n",
    "x_test_n = scaler.fit_transform(np.array(x_test).T)\n",
    "x_test_n = torch.Tensor(x_test_n)\n",
    "l = [len(y) for y in y_test]\n",
    "q = np.max(l)\n",
    "y_test_p = torch.zeros(len(y_test),q)\n",
    "for i,y in enumerate(y_test):\n",
    "    y_test_p[i,:] = torch.cat([y,torch.zeros(q-y.shape[0])])\n",
    "test_data = TensorDataset(x_test_n.T,y_test_p.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6ba5552a2faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#test_data = torch.load('./Saved Data\\\\test_data_int_comp.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msize_vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1bd5b5e53e16>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1bd5b5e53e16>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/EQLearner/env/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#torch.save(train_data, 'train_data_int_comp.pt') \n",
    "#torch.save(test_data, 'test_data_int_comp.pt') \n",
    "#train_data = torch.load('./Saved Data\\\\train_data_int_comp.pt')\n",
    "#test_data = torch.load('./Saved Data\\\\test_data_int_comp.pt')\n",
    "\n",
    "for i in train_data:\n",
    "    size_vec=i[1]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, valid_idx, train_idx = dataset_loader(train_data,test_data, batch_size = 2000, valid_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 30):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.tok_linear = nn.Linear(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "\n",
    "        src = torch.unsqueeze(src,dim = 2)\n",
    "        \n",
    "        tok_embedded = ((self.tok_linear(src)))\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, src len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, src len]\n",
    "        \n",
    "        #begin convolutional blocks...\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, src len]\n",
    "\n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        #...end convolutional blocks\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved = [batch size, src len, emb dim]\n",
    "        \n",
    "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 trg_pad_idx, \n",
    "                 device,\n",
    "                 max_length = 36):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        #conved = [batch size, hid dim, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved_emb = [batch size, trg len, emb dim]\n",
    "        \n",
    "        combined = (conved_emb + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, trg len, emb dim]\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        #energy = [batch size, trg len, src len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        #attention = [batch size, trg len, src len]\n",
    "            \n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, emd dim]\n",
    "        \n",
    "        #convert from emb dim -> hid dim\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #apply residual connection\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        #attended_combined = [batch size, hid dim, trg len]\n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, trg len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = [batch size, trg len, emb dim]\n",
    "        #pos_embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, trg len]\n",
    "        \n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #apply dropout\n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            #need to pad so decoder can't \"cheat\"\n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
    "        \n",
    "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(padded_conv_input)\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, trg len]\n",
    "            \n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #calculate attention\n",
    "            attention, conved = self.calculate_attention(embedded, \n",
    "                                                         conved, \n",
    "                                                         encoder_conved, \n",
    "                                                         encoder_combined)\n",
    "            \n",
    "            #attention = [batch size, trg len, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "        #conved = [batch size, trg len, emb dim]\n",
    "            \n",
    "        output = self.fc_out((conved))\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
    "           \n",
    "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
    "        #encoder_conved is output from final encoder conv. block\n",
    "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
    "        #  positional embeddings \n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "            \n",
    "        #encoder_conved = [batch size, src len, emb dim]\n",
    "        #encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #calculate predictions of next words\n",
    "        #output is a batch of predictions for each word in the trg sentence\n",
    "        #attention a batch of attention scores across the src sentence for \n",
    "        #  each word in the trg sentence\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #attention = [batch size, trg len - 1, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,109,904 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 16\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "ENC_LAYERS = 2 # number of conv. blocks in encoder 10 original\n",
    "DEC_LAYERS = 2 # number of conv. blocks in decoder 10 original\n",
    "ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "TRG_PAD_IDX = 0\n",
    "    \n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, noise_Y=False,sigma = 0.1):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        if noise_Y:\n",
    "            src = batch[0] + torch.from_numpy(sigma*np.random.randn(batch[0].shape[0],30)).float().cuda()\n",
    "            trg = batch[1]\n",
    "        else:\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "    \n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "       \n",
    "\n",
    "        \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-265e7f37d7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-dd1d27c94e6c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, noise_Y, sigma)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnoise_Y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/EQLearner/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/EQLearner/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/EQLearner/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/EQLearner/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1bd5b5e53e16>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1bd5b5e53e16>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/EQLearner/env/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "N_EPOCHS =100\n",
    "\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP,noise_Y = False,sigma = 0.05)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'cnn.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}) | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}) |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('cnn.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [ ]:\n",
    "def num_evaluation(model_o,y_test,n):\n",
    "    c = True\n",
    "    i=0\n",
    "    while c:\n",
    "        try:\n",
    "            fun = lambdify(x,tokenization.get_string(np.array([12]+list(model_o.detach().argmax(2)[1:-i,n].cpu()))))\n",
    "            print('Prediction: ', tokenization.get_string(np.array([12]+list(model_o.detach().argmax(2)[1:-i,n].cpu()))))\n",
    "            c = False\n",
    "        except:\n",
    "            i+=1\n",
    "    print('Ground Truth: ', tokenization.get_string(y_test[n].numpy()))\n",
    "    trueth = lambdify(x,tokenization.get_string(y_test[n].numpy()))\n",
    "    enlarged_supp = np.arange(0.1,15,0.1)\n",
    "    supp = np.arange(0.1,3.1,0.1)\n",
    "    MSE = np.mean((trueth(enlarged_supp)-fun(enlarged_supp))**2)\n",
    "    print('MSE: ', MSE)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.plot(enlarged_supp,trueth(enlarged_supp), label = 'ground-true')\n",
    "    plt.plot(enlarged_supp,fun(enlarged_supp), label = 'prediction')\n",
    "    plt.plot(supp,x_test[n], label = 't')\n",
    "    plt.legend()\n",
    "#    plt.ylim(0,10)\n",
    "#    plt.xlim(0,3)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    a = model(x_test_n.T[:1000].cuda(), y_test_p[:1000].long().cuda(),0)\n",
    "num_evaluation(a,y_test,199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [ ]:\n",
    "def user_eval(support, model, y_test_p):\n",
    "    model.eval()\n",
    "    expression = input()\n",
    "    av = lambdify(x,expression)\n",
    "    xxx = av(support)\n",
    "    scaler = MinMaxScaler()\n",
    "    xxx_n = scaler.fit_transform(np.expand_dims(xxx,0).T)\n",
    "    #print(xxx_n.T)\n",
    "    x_new = torch.from_numpy(xxx_n.T).float().cuda()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_new,y_test_p[:1].long().cuda())\n",
    "        pred = pred[0].permute(1,0,2)\n",
    "        print(pred.shape)\n",
    "        print(tokenization.get_string(np.array([12]+list(pred.detach().argmax(2)[1:].cpu()))))\n",
    "        c = True\n",
    "        i=0\n",
    "        while c:\n",
    "            try:\n",
    "                fun = lambdify(x,tokenization.get_string(np.array([12]+list(pred.detach().argmax(2)[1:-i].cpu()))))\n",
    "                print('Prediction: ', tokenization.get_string(np.array([12]+list(pred.detach().argmax(2)[1:-i].cpu()))))\n",
    "                c = False\n",
    "            except:\n",
    "                i+=1\n",
    "                if i >30:\n",
    "                    break\n",
    "            \n",
    "        print('Ground Truth: ', expression)\n",
    "        ss = np.arange(0.1,15.1,0.1)\n",
    "        enlarged_supp = np.arange(0.1,3.1,0.1)\n",
    "        MSE = np.mean((av(enlarged_supp)-fun(enlarged_supp))**2)\n",
    "        print('MSE: ', MSE)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.plot(enlarged_supp,scaler.fit_transform(np.expand_dims(av(enlarged_supp),1))[:,0], label = 'ground-true')\n",
    "        plt.plot(enlarged_supp,scaler.fit_transform(np.expand_dims(fun(enlarged_supp),1))[:,0], label = 'prediction')\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.plot(ss,(np.expand_dims(av(ss),1))[:,0], label = 'ground-true')\n",
    "        plt.plot(ss,(np.expand_dims(fun(ss),1))[:,0], label = 'prediction')\n",
    "        plt.legend()\n",
    "        #plt.ylim(-4,10)\n",
    "        #print(scaler.fit_transform(np.expand_dims(fun(support),1)).T)\n",
    "        return pred\n",
    "\n",
    "prob = user_eval(support, model,size_vec.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
