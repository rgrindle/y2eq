# The perils and pitfalls of symbolic regression

The ever-growing accumulation of data makes automated distillation of understandable models from that data ever-more desirable. Deriving equations directly from data using symbolic regression, as performed by genetic programming, continues its appeal due to its algorithmic simplicity and lack of assumptions about equation form. However, few models besides a sequence-to-sequence approach to symbolic regression, introduced in 2020 [1] that we call y2eq, have been shown capable of transfer learning: the ability to rapidly distill equations successfully on new data from a previously unseen domain, due to experience performing this distillation on other domains. In order to improve this model, it is necessary to understand the key challenges associated with it. We have identified three important challenges: corpus, coefficient, and cost. The challenge of devising a training corpus stems from the hierarchical nature of the data since the corpus should not be considered as a collection of equations but rather as a collection of functional forms and instances of those functional forms. The challenge of choosing appropriate coefficients for functional forms compounds the corpus challenge and presents further challenges during evaluation of trained models due to the potential for similarity between instances of different functional forms. The challenge with cost functions (used to train the model) is mainly the choice between numeric cost (compares y-values) and symbolic cost (compares written functional forms). This code repository re-implements the neural network from [1]. The repository can be used to provide evidence for the existence of the corpus, coefficient, and cost challenges. We hope that this repository can be used to initiate improvements to this already promising symbolic regression model.

With this code, all expimental results from here (TODO: link to thesis) can be recreated.


[1] Biggio, Luca, Tommaso Bendinelli, Aurelien Lucchi, and Giambattista Parascandolo. "A seq2seq approach to symbolic regression." In Learning Meets Combinatorial Algorithms at NeurIPS2020. 2020.


## Getting started
I am running this code on Python 3.7.4 with the following dependencies.

numpy==1.17.1 pandas==0.25.1 tensorflow==2.3.1 sympy==1.6.2 scipy==1.4.1 torch==1.7.1 matplotlib==2.2.3

The [EQLearner respository](https://github.com/SymposiumOrganization/EQLearner) is also used to help generated the datasets. Note that you will need to add some empty `__init__.py` files to install EQLearner.

### Install
```
mkdir SRvGD
cd SRvGD
clone https://github.com/rgrindle/y2eq.git
mv y2eq/* y2eq/.git .
pip install .
```

## Examples
The following examples can be used to recreate results from (TODO: link to thesis here)

### Example 1: Generate dataset
In this example, a training dataset of 50,000 observations is generated using 1,000 functional forms. Each functional form is involved in roughly 50 observations. Each observation is (y, f) where y is a set of y-values corresponding to the functional form f.
```
cd SRvGD/src/srvgd/data_gathering/
python generate_dataset_from_functional_forms.py
```
This creates SRvGD/datasets/dataset_train_ff1000.pt

Now that this dataset has been created you can run any of the dataset scripts (inside SRvGD/src/srvgd/data_gathering/) that begin with the "rebuild". There are also some datasets that are generated using more than one script. These scripts are numbered to indicate the order in which they should be run. See the "PURPOSE" and "NOTES" of any script to understand what dataset will be generated by running the script.

### Example 2a: Train the neural network (transformer)
To train the y2eq-transformer model on the dataset created by Example 1, 
```
cd SRvGD/src/srvgd/architecture/transformer/
python y2eq_transformer.py
```
The model will be saved as SRvGD/models/y2eq_transformer.pt (model on final epoch) and SRvGD/models/BEST_y2eq_transformer.pt (best model according to validation loss). There are several variations of y2eq-transformer that can be trained just like this one. These variations can be found in SRvGD/src/srvgd/architecture/transformer/.

In all these transform models, checkpointing can be used by changing `checkpoint_filename` and `model_name`.

### Example 2b: Train the neural network (convolutional seq2seq)
To train the y2eq-transformer model on the dataset created by Example 1, 
```
cd train_y2eq_conv/
python train_control.py --dataset dataset_train_ff1000.py --input_dim 1
```
The model will be saved in SRvGD/models/. The model saved is the best model based on validation loss. The model's name will begin with y2eq_ and continue with the cmd line arguments that were provided to it. Use python `train_control.py --help` for a full list of arguments. 

### Example 3: Evaluate trained the neural network
Once y2eq has been trained (see Example 2), y2eq can be evaluated by
```
cd SRvGD/src/srvgd/eval_y2eq-transformer-fixed-fixed/
python 00_eval_y2eq-transformer-fixed-fixed.py
python 01_eval_y2eq-transformer-fixed-fixed.py
for i in {0..999}
do
  python 02_eval_y2eq-transformer-fixed-fixed_vacc.py
done
python 03_eval_y2eq-transformer-fixed-fixed_vacc.py
```
First, 00 creates data to evaluate y2eq-transformer, then 01 records the output functional forms, then 02 computes root mean squared errors (RMSE) of these functional forms, and finally 03 groups all RMSEs into one file: 02_rmse_150.csv. Note that when there is not 02_..._vacc.py script then the 02_... file that does not end in _vacc should be used without looping. In such cases, BFGS (or L-BFGS-B) is probably not being used and so the output file is 02_rmse.csv. 

There are many eval_y2eq... folders and scripts. Each script has a PURPOSE and NOTE that will help explain what they are used for. The follow is an explanation of the naming convention.

Names often include fixed, different, or normal. These words refer to the x-values in the dataset. fixed means that the x-values are fix at x = {0.1, 0.2, ..., 3.0} (just like original paper). different means x-values are picked randomly from a uniform distribution on \[0.1, 3.1). normal means x-values are picked randomly from a normal distribution.

Often fixed, different, or normal appear in a combination of two of these words. The first instance of one of these names indicates how the model was trained and the second indicates how the model was tested.

transformer is only included in the name if the model is a transformer. Otherwise you can assume that the model is a convolutional seq2seq model.

Don't forget to look at PURPOSE and NOTES in the script to get a better understanding of what is being evaluated.

